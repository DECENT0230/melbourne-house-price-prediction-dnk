import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
import joblib

# Load preprocessed data
data = pd.read_csv('preprocessed_clean_v4.csv')

# Features and target
features = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Bedroom_Discrepancy']
X = data[features]
y = data['Price']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit scaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to evaluate and print metrics
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"\n{model_name} Performance:")
    print(f"MAE: ${mae:,.2f}")
    print(f"RMSE: ${rmse:,.2f}")
    print(f"R²: {r2:.4f}")
    return mae, rmse, r2

# Random Forest: Expanded hyperparameter tuning
rf = RandomForestRegressor(random_state=42)
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'max_features': ['auto', 'sqrt', 0.5]
}
random_search_rf = RandomizedSearchCV(rf, param_dist, n_iter=20, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
random_search_rf.fit(X_train_scaled, y_train)

# Best Random Forest model
best_rf = random_search_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test_scaled)
rf_metrics = evaluate_model(y_test, y_pred_rf, "Random Forest")

# Random Forest with log-transformed Price
y_train_log = np.log1p(y_train)
random_search_rf.fit(X_train_scaled, y_train_log)
best_rf_log = random_search_rf.best_estimator_
y_pred_rf_log = np.expm1(best_rf_log.predict(X_test_scaled))
rf_log_metrics = evaluate_model(y_test, y_pred_rf_log, "Random Forest (Log-Transformed)")

# XGBoost
xgb = XGBRegressor(random_state=42)
param_dist_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.7, 0.8, 1.0]
}
random_search_xgb = RandomizedSearchCV(xgb, param_dist_xgb, n_iter=20, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)
random_search_xgb.fit(X_train_scaled, y_train)

# Best XGBoost model
best_xgb = random_search_xgb.best_estimator_
y_pred_xgb = best_xgb.predict(X_test_scaled)
xgb_metrics = evaluate_model(y_test, y_pred_xgb, "XGBoost")

# Linear Regression (baseline)
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
y_pred_lr = lr.predict(X_test_scaled)
lr_metrics = evaluate_model(y_test, y_pred_lr, "Linear Regression")

# Select best model (highest R²)
models = {
    'Random Forest': (best_rf, rf_metrics),
    'Random Forest (Log)': (best_rf_log, rf_log_metrics),
    'XGBoost': (best_xgb, xgb_metrics),
    'Linear Regression': (lr, lr_metrics)
}
best_model_name = max(models, key=lambda k: models[k][1][2])
best_model, best_metrics = models[best_model_name]

# Save the best model and scaler
joblib.dump(best_model, 'best_model.joblib')
joblib.dump(scaler, 'scaler.joblib')

print(f"\n✅ Saved best model ({best_model_name}) to 'best_model.joblib'")
print("✅ Saved fitted scaler to 'scaler.joblib'")
